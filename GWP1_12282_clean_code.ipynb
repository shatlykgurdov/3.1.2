{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcApQJcZ4/vGRjrYSKTMLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shatlykgurdov/3.1.2/blob/main/GWP1_12282_clean_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zOUKFje55PX3"
      },
      "outputs": [],
      "source": [
        "# Group 12282 GWP1\n",
        "# -------------------------\n",
        "\n",
        "# Common imports\n",
        "# -------------------------\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import yfinance as yf\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "\n",
        "# Problem 1d\n",
        "# -----------------------------\n",
        "# Settings & true parameters\n",
        "# -----------------------------\n",
        "np.random.seed(123)\n",
        "\n",
        "a_true = 1.0\n",
        "b_true = 2.0\n",
        "c_true = 3.0\n",
        "sigma_e = 1.0\n",
        "rho = 0.7            # correlation between X and Z\n",
        "sample_sizes = [50, 200, 1000]\n",
        "n_sim = 1000         # number of simulations per sample size\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: simulate once and estimate models\n",
        "# -----------------------------\n",
        "def simulate_once(N):\n",
        "    \"\"\"\n",
        "    Simulate one dataset from:\n",
        "        Y = a + b*X + c*Z + e\n",
        "    where X and Z are correlated, then estimate:\n",
        "      - Full model: Y ~ X + Z\n",
        "      - Omitted model: Y ~ X\n",
        "    Returns b_hat_full, b_hat_omit.\n",
        "    \"\"\"\n",
        "    # Generate X\n",
        "    X = np.random.normal(0, 1, size=N)\n",
        "    # Generate noise for Z\n",
        "    u = np.random.normal(0, 1, size=N)\n",
        "    # Make Z correlated with X\n",
        "    Z = rho * X + np.sqrt(1 - rho**2) * u\n",
        "    # Error term\n",
        "    e = np.random.normal(0, sigma_e, size=N)\n",
        "    # True model\n",
        "    Y = a_true + b_true * X + c_true * Z + e\n",
        "\n",
        "    # Full model: Y ~ X + Z\n",
        "    X_full = np.column_stack([X, Z])\n",
        "    X_full = sm.add_constant(X_full)\n",
        "    model_full = sm.OLS(Y, X_full).fit()\n",
        "    b_hat_full = model_full.params[1]   # coefficient of X\n",
        "\n",
        "    # Omitted model: Y ~ X\n",
        "    X_omit = sm.add_constant(X)\n",
        "    model_omit = sm.OLS(Y, X_omit).fit()\n",
        "    b_hat_omit = model_omit.params[1]\n",
        "\n",
        "    return b_hat_full, b_hat_omit, model_full, model_omit\n",
        "\n",
        "# -----------------------------\n",
        "# Part 1: Single example run\n",
        "# -----------------------------\n",
        "print(\"=== Single example run (N = 200) ===\")\n",
        "b_full, b_omit, model_full, model_omit = simulate_once(200)\n",
        "print(\"True b:\", b_true)\n",
        "print(\"Full model b_hat:\", round(b_full, 4))\n",
        "print(\"Omitted model b_hat:\", round(b_omit, 4))\n",
        "print(\"\\nFull model summary:\")\n",
        "print(model_full.summary())\n",
        "print(\"\\nOmitted model summary:\")\n",
        "print(model_omit.summary())\n",
        "\n",
        "# -----------------------------\n",
        "# Part 2: Many simulations to study bias & variance\n",
        "# -----------------------------\n",
        "results = {}\n",
        "\n",
        "for N in sample_sizes:\n",
        "    b_full_list = []\n",
        "    b_omit_list = []\n",
        "\n",
        "    for _ in range(n_sim):\n",
        "        b_f, b_o, _, _ = simulate_once(N)\n",
        "        b_full_list.append(b_f)\n",
        "        b_omit_list.append(b_o)\n",
        "\n",
        "    results[N] = {\n",
        "        \"b_full_mean\": np.mean(b_full_list),\n",
        "        \"b_full_std\": np.std(b_full_list),\n",
        "        \"b_omit_mean\": np.mean(b_omit_list),\n",
        "        \"b_omit_std\": np.std(b_omit_list),\n",
        "        \"b_full_list\": b_full_list,\n",
        "        \"b_omit_list\": b_omit_list,\n",
        "    }\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n=== Simulation summary over\", n_sim, \"runs ===\")\n",
        "print(\"True b:\", b_true)\n",
        "\n",
        "for N in sample_sizes:\n",
        "    print(f\"\\nSample size N = {N}\")\n",
        "    print(\"Full model:   mean(b_hat) =\",\n",
        "          round(results[N][\"b_full_mean\"], 3),\n",
        "          \", std(b_hat) =\",\n",
        "          round(results[N][\"b_full_std\"], 3))\n",
        "    print(\"Omitted model: mean(b_hat) =\",\n",
        "          round(results[N][\"b_omit_mean\"], 3),\n",
        "          \", std(b_hat) =\",\n",
        "          round(results[N][\"b_omit_std\"], 3))\n",
        "\n",
        "# -----------------------------\n",
        "# Part 3: Plot distribution for one N\n",
        "# -----------------------------\n",
        "N_plot = 200   # choose which N to visualize\n",
        "b_full_list = results[N_plot][\"b_full_list\"]\n",
        "b_omit_list = results[N_plot][\"b_omit_list\"]\n",
        "\n",
        "plt.hist(b_full_list, bins=30, alpha=0.5, label=\"Full model b_hat\")\n",
        "plt.hist(b_omit_list, bins=30, alpha=0.5, label=\"Omitted model b_hat\")\n",
        "plt.axvline(b_true, linestyle=\"--\", label=\"True b\")\n",
        "plt.title(f\"Distribution of b_hat (N = {N_plot})\")\n",
        "plt.xlabel(\"b_hat\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# GWP1 Problem 2b\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# 1) Generate clean linear data\n",
        "n_samples = 100\n",
        "X = rng.uniform(0, 10, size=(n_samples, 1))\n",
        "noise = rng.normal(0, 1, size=n_samples)\n",
        "y = 3 * X.ravel() + 5 + noise # true slope = 3, intercept = 5\n",
        "\n",
        "# Fit linear regression on clean data\n",
        "lin_clean = LinearRegression()\n",
        "lin_clean.fit(X, y)\n",
        "y_pred_clean = lin_clean.predict(X)\n",
        "\n",
        "print(\"CLEAN DATA\")\n",
        "print(\"Intercept:\", lin_clean.intercept_)\n",
        "print(\"Slope:\", lin_clean.coef_[0])\n",
        "print(\"R2:\", r2_score(y, y_pred_clean))\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_clean))\n",
        "\n",
        "# 2) Make a copy and inject outliers\n",
        "X_out = X.copy()\n",
        "y_out = y.copy()\n",
        "\n",
        "n_outliers = 5\n",
        "idx = rng.choice(n_samples, size=n_outliers, replace=False)\n",
        "\n",
        "# Extreme y-values at existing X\n",
        "y_out[idx] = y_out[idx] + rng.normal(40, 5, size=n_outliers)\n",
        "\n",
        "# Fit linear regression on contaminated data\n",
        "lin_out = LinearRegression()\n",
        "lin_out.fit(X_out, y_out)\n",
        "y_pred_out = lin_out.predict(X_out)\n",
        "\n",
        "print(\"\\nWITH OUTLIERS\")\n",
        "print(\"Intercept:\", lin_out.intercept_)\n",
        "print(\"Slope:\", lin_out.coef_[0])\n",
        "print(\"R2:\", r2_score(y_out, y_pred_out))\n",
        "print(\"MSE:\", mean_squared_error(y_out, y_pred_out))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Clean data plot\n",
        "ax1.scatter(X, y, alpha=0.6, color='blue', label='Data')\n",
        "x_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "ax1.plot(x_range, lin_clean.predict(x_range), color='red', linewidth=2, label='Fit')\n",
        "ax1.set_title('Clean Data')\n",
        "ax1.set_xlabel('X')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Outlier data plot\n",
        "ax2.scatter(X_out, y_out, alpha=0.6, color='blue', label='Data')\n",
        "ax2.plot(x_range, lin_out.predict(x_range), color='red', linewidth=2, label='Fit')\n",
        "ax2.set_title('With Outliers')\n",
        "ax2.set_xlabel('X')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Clean data model with significance\n",
        "X_clean_sm = sm.add_constant(X)  # Adds intercept column\n",
        "model_clean = sm.OLS(y, X_clean_sm).fit()\n",
        "print(\"\\nCLEAN DATA MODEL SUMMARY:\")\n",
        "print(model_clean.summary())\n",
        "\n",
        "# Outlier data model with significance\n",
        "X_out_sm = sm.add_constant(X_out)\n",
        "model_out = sm.OLS(y_out, X_out_sm).fit()\n",
        "print(\"\\nOUTLIER DATA MODEL SUMMARY:\")\n",
        "print(model_out.summary())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Problem 3a – Model Selection for Y ~ Z1, Z2, Z3, Z4, Z5\n",
        "# Group 12282 (even) → FE-GWP1_model_selection_2.csv\n",
        "\n",
        "# =========================\n",
        "# 1. Load the dataset\n",
        "# =========================\n",
        "df = pd.read_csv(\"FE-GWP1_model_selection_2.csv\")\n",
        "\n",
        "# Clean column names: remove leading/trailing spaces\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "print(\"First 5 rows of dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\nColumns in dataset:\", df.columns.tolist())\n",
        "\n",
        "# Now columns should be: 'Y', 'Z1', 'Z2', 'Z3', 'Z4', 'Z5'\n",
        "y = df[\"Y\"]\n",
        "X_all = df[[\"Z1\", \"Z2\", \"Z3\", \"Z4\", \"Z5\"]]\n",
        "candidate_predictors = list(X_all.columns)\n",
        "\n",
        "# Helper function to fit OLS model with given predictors\n",
        "def fit_model(predictors):\n",
        "    X = sm.add_constant(X_all[list(predictors)])\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    return model\n",
        "\n",
        "# =========================\n",
        "# 2. Approach 1: All-subsets model selection\n",
        "#    (Adjusted R², AIC, BIC)\n",
        "# =========================\n",
        "results = []\n",
        "\n",
        "for k in range(1, len(candidate_predictors) + 1):\n",
        "    for subset in itertools.combinations(candidate_predictors, k):\n",
        "        model = fit_model(subset)\n",
        "        results.append({\n",
        "            \"predictors\": subset,\n",
        "            \"adj_R2\": model.rsquared_adj,\n",
        "            \"AIC\": model.aic,\n",
        "            \"BIC\": model.bic\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Top models by each criterion\n",
        "best_by_adjR2 = results_df.sort_values(\"adj_R2\", ascending=False).head(5)\n",
        "best_by_AIC   = results_df.sort_values(\"AIC\", ascending=True).head(5)\n",
        "best_by_BIC   = results_df.sort_values(\"BIC\", ascending=True).head(5)\n",
        "\n",
        "print(\"\\n=== Top 5 models by Adjusted R² ===\")\n",
        "print(best_by_adjR2)\n",
        "\n",
        "print(\"\\n=== Top 5 models by AIC ===\")\n",
        "print(best_by_AIC)\n",
        "\n",
        "print(\"\\n=== Top 5 models by BIC ===\")\n",
        "print(best_by_BIC)\n",
        "\n",
        "# Choose one \"best\" model by BIC\n",
        "best_model_row = best_by_BIC.iloc[0]\n",
        "best_predictors = list(best_model_row[\"predictors\"])\n",
        "print(\"\\n>>> Chosen best model by BIC has predictors:\", best_predictors)\n",
        "\n",
        "best_model = fit_model(best_predictors)\n",
        "print(\"\\n=== Summary of best model by BIC ===\")\n",
        "print(best_model.summary())\n",
        "\n",
        "# =========================\n",
        "# 3. Approach 2: Forward stepwise selection (AIC)\n",
        "# =========================\n",
        "def forward_stepwise_selection(X, y, verbose=True):\n",
        "    remaining = list(X.columns)\n",
        "    selected = []\n",
        "    current_score = np.inf\n",
        "    best_new_score = np.inf\n",
        "\n",
        "    while remaining:\n",
        "        scores_with_candidates = []\n",
        "\n",
        "        for candidate in remaining:\n",
        "            predictors = selected + [candidate]\n",
        "            X_candidate = sm.add_constant(X[predictors])\n",
        "            model = sm.OLS(y, X_candidate).fit()\n",
        "            scores_with_candidates.append((model.aic, candidate))\n",
        "\n",
        "        # Pick candidate with lowest AIC\n",
        "        scores_with_candidates.sort()\n",
        "        best_new_score, best_candidate = scores_with_candidates[0]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Trying to add {best_candidate}: AIC = {best_new_score:.3f}\")\n",
        "\n",
        "        if best_new_score < current_score - 1e-6:  # improvement\n",
        "            remaining.remove(best_candidate)\n",
        "            selected.append(best_candidate)\n",
        "            current_score = best_new_score\n",
        "            if verbose:\n",
        "                print(f\"  -> Added {best_candidate}, new AIC = {current_score:.3f}\")\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"No further AIC improvement. Stopping forward selection.\")\n",
        "            break\n",
        "\n",
        "    return selected, current_score\n",
        "\n",
        "print(\"\\n=== Forward Stepwise Selection (AIC) ===\")\n",
        "forward_predictors, forward_aic = forward_stepwise_selection(X_all, y, verbose=True)\n",
        "print(\"\\n>>> Forward stepwise selected predictors:\", forward_predictors)\n",
        "print(\"Final AIC from forward selection:\", forward_aic)\n",
        "\n",
        "X_forward = sm.add_constant(X_all[forward_predictors])\n",
        "forward_model = sm.OLS(y, X_forward).fit()\n",
        "print(\"\\n=== Summary of forward stepwise model ===\")\n",
        "print(forward_model.summary())\n",
        "\n",
        "# =========================\n",
        "# 4. Approach 3: Backward elimination (BIC)\n",
        "# =========================\n",
        "def backward_elimination(X, y, verbose=True, criterion=\"BIC\"):\n",
        "    predictors = list(X.columns)\n",
        "\n",
        "    # Start with full model\n",
        "    X_full = sm.add_constant(X[predictors])\n",
        "    model_full = sm.OLS(y, X_full).fit()\n",
        "    if criterion == \"AIC\":\n",
        "        best_score = model_full.aic\n",
        "    elif criterion == \"BIC\":\n",
        "        best_score = model_full.bic\n",
        "    else:\n",
        "        raise ValueError(\"criterion must be 'AIC' or 'BIC'\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nStart backward elimination with full model {predictors}, {criterion} = {best_score:.3f}\")\n",
        "\n",
        "    improved = True\n",
        "    while improved and len(predictors) > 1:\n",
        "        scores_with_candidates = []\n",
        "\n",
        "        for candidate in predictors:\n",
        "            trial_predictors = [p for p in predictors if p != candidate]\n",
        "            X_trial = sm.add_constant(X[trial_predictors])\n",
        "            trial_model = sm.OLS(y, X_trial).fit()\n",
        "            score = trial_model.aic if criterion == \"AIC\" else trial_model.bic\n",
        "            scores_with_candidates.append((score, candidate, trial_predictors))\n",
        "\n",
        "        scores_with_candidates.sort()\n",
        "        best_new_score, worst_predictor, best_predictor_set = scores_with_candidates[0]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Trying to remove {worst_predictor}: {criterion} = {best_new_score:.3f}\")\n",
        "\n",
        "        if best_new_score < best_score - 1e-6:\n",
        "            predictors = best_predictor_set\n",
        "            best_score = best_new_score\n",
        "            if verbose:\n",
        "                print(f\"  -> Removed {worst_predictor}, new {criterion} = {best_score:.3f}\")\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"No further improvement. Stopping backward elimination.\")\n",
        "            improved = False\n",
        "\n",
        "    final_X = sm.add_constant(X[predictors])\n",
        "    final_model = sm.OLS(y, final_X).fit()\n",
        "    return predictors, best_score, final_model\n",
        "\n",
        "print(\"\\n=== Backward Elimination (BIC) ===\")\n",
        "backward_predictors, backward_score, backward_model = backward_elimination(X_all, y, verbose=True, criterion=\"BIC\")\n",
        "print(\"\\n>>> Backward elimination (BIC) selected predictors:\", backward_predictors)\n",
        "print(\"Final BIC from backward elimination:\", backward_score)\n",
        "print(\"\\n=== Summary of backward elimination model ===\")\n",
        "print(backward_model.summary())\n",
        "\n",
        "# =========================\n",
        "# 5. Final comparison info\n",
        "# =========================\n",
        "print(\"\\n================ FINAL SUMMARY ================\")\n",
        "print(\"Best model by BIC (all-subsets) predictors:\", best_predictors)\n",
        "print(\"Forward stepwise selected predictors:\", forward_predictors)\n",
        "print(\"Backward elimination (BIC) selected predictors:\", backward_predictors)\n",
        "print(\"Use these results to justify your final chosen model in the report.\")\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GWP1 — Problem 5b and 5c\n",
        "# ============================================================\n",
        "\n",
        "# ============================================================\n",
        "# 5b) Use real economic data for equities and illustrate if the\n",
        "#     time series you chose has a unit root.\n",
        "# ============================================================\n",
        "\n",
        "# ---- 5b.1: Load real equity price data ----\n",
        "# Option A : use yfinance (works in Colab / many environments)\n",
        "# If yfinance is not installed, uncomment the next line:\n",
        "# !pip -q install yfinance\n",
        "\n",
        "\n",
        "TICKER = \"AAPL\"            # change if you want: \"MSFT\", \"^GSPC\" (S&P500), \"TSLA\", etc.\n",
        "START = \"2018-01-01\"\n",
        "END   = \"2024-01-01\"\n",
        "\n",
        "data = yf.download(TICKER, start=START, end=END, progress=False)\n",
        "prices = data[\"Close\"].dropna()\n",
        "\n",
        "print(f\"5b) Loaded {len(prices)} daily closing prices for {TICKER} from {prices.index.min().date()} to {prices.index.max().date()}\")\n",
        "\n",
        "# ---- 5b.2: Plot the price series ----\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(prices)\n",
        "plt.title(f\"5b) {TICKER} Daily Closing Prices\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- 5b.3: Run ADF test on levels (prices) ----\n",
        "# H0: unit root (non-stationary)\n",
        "# H1: stationary\n",
        "adf_levels = adfuller(prices, autolag=\"AIC\")\n",
        "\n",
        "print(\"\\n5b) ADF test on PRICE LEVELS\")\n",
        "print(\"ADF statistic:\", adf_levels[0])\n",
        "print(\"p-value      :\", adf_levels[1])\n",
        "print(\"Used lags     :\", adf_levels[2])\n",
        "print(\"N obs         :\", adf_levels[3])\n",
        "print(\"Critical vals :\", adf_levels[4])\n",
        "\n",
        "if adf_levels[1] < 0.05:\n",
        "    print(\"Conclusion (5%): Reject H0 → evidence against unit root (stationary).\")\n",
        "else:\n",
        "    print(\"Conclusion (5%): Fail to reject H0 → consistent with a unit root (non-stationary).\")\n",
        "\n",
        "# ---- 5b.4 (optional but very helpful): show that first differences are usually stationary ----\n",
        "returns_diff = prices.diff().dropna()  # first difference of price (not log returns)\n",
        "adf_diff = adfuller(returns_diff, autolag=\"AIC\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(returns_diff)\n",
        "plt.title(f\"5b) First Difference of {TICKER} Prices\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Δ Price\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n5b) ADF test on FIRST DIFFERENCES (ΔPrice)\")\n",
        "print(\"ADF statistic:\", adf_diff[0])\n",
        "print(\"p-value      :\", adf_diff[1])\n",
        "print(\"Critical vals :\", adf_diff[4])\n",
        "\n",
        "if adf_diff[1] < 0.05:\n",
        "    print(\"Conclusion (5%): Reject H0 → differenced series is likely stationary.\")\n",
        "else:\n",
        "    print(\"Conclusion (5%): Fail to reject H0 → differenced series may still be non-stationary.\")\n",
        "\n",
        "# ============================================================\n",
        "# 5c) Why are we concerned about a unit root (ρ=1) and not a\n",
        "#     root of 1.5? Use simulations to compare the two.\n",
        "# ============================================================\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---- 5c.1: Simulation settings ----\n",
        "T = 200                # number of time points\n",
        "sigma_eps = 1.0        # shock volatility\n",
        "\n",
        "# Unit root (random walk): y_t = y_{t-1} + eps_t   (ρ = 1)\n",
        "rho_unit = 1.0\n",
        "\n",
        "# Explosive root: y_t = ρ y_{t-1} + eps_t          (ρ = 1.5)\n",
        "rho_explosive = 1.5\n",
        "\n",
        "eps = np.random.normal(0, sigma_eps, size=T)\n",
        "\n",
        "# ---- 5c.2: Generate the two processes ----\n",
        "y_unit = np.zeros(T)\n",
        "y_explosive = np.zeros(T)\n",
        "\n",
        "for t in range(1, T):\n",
        "    y_unit[t] = rho_unit * y_unit[t-1] + eps[t]         # random walk\n",
        "    y_explosive[t] = rho_explosive * y_explosive[t-1] + eps[t]  # explosive\n",
        "\n",
        "# ---- 5c.3: Plot both series for visual comparison ----\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(y_unit, label=\"5c) Unit Root (ρ = 1)\")\n",
        "plt.plot(y_explosive, label=\"5c) Explosive Root (ρ = 1.5)\")\n",
        "plt.title(\"5c) Simulated Time Series: Unit Root vs Explosive Root\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- 5c.4:  plot: same plot but with y-axis limits for unit root ----\n",
        "# This helps show how explosive series quickly dominates the scale.\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(y_unit, label=\"Unit Root (ρ = 1)\")\n",
        "plt.title(\"5c) Unit Root Process Alone (ρ = 1)\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n5c) Notes for interpretation:\")\n",
        "print(\"- ρ = 1 (unit root) behaves like a random walk: shocks have permanent effects, series wanders but does not explode deterministically.\")\n",
        "print(\"- ρ = 1.5 is explosive: values grow rapidly in magnitude, typically not resembling real financial/economic series over long samples.\")\n",
        "print(\"- This is why unit roots are a key concern: they create non-stationarity without unrealistic explosive growth.\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}